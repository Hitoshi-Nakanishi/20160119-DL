{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chainerを用いた日英翻訳機\n",
    "\n",
    "\n",
    "- 参考: http://qiita.com/odashi_t/items/a1be7c4964fbea6a116e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import codecs\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import MeCab\n",
    "mt = MeCab.Tagger('-Owakati')\n",
    "\n",
    "import six\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from chainer import computational_graph as c\n",
    "from chainer import cuda, Variable, FunctionSet, optimizers\n",
    "import chainer.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "## 一般的なmodel\n",
    "$\\begin{align}\n",
    "{\\bf h}_n & = \\tanh \\bigl( W_{xh} \\cdot {\\bf x}_n + W_{hh} \\cdot {\\bf h}_{n-1} \\bigr), \\\\\n",
    "{\\bf y}_n & = {\\rm softmax} \\bigl( W_{hy} \\cdot {\\bf h}_n \\bigr)\n",
    "\\end{align}$\n",
    "\n",
    "## 今回実装するmodel\n",
    "$\\begin{align}\n",
    "{\\bf i}_n & = \\tanh \\bigl( W_{xi} \\cdot {\\bf x}_n \\bigr), \\\\\n",
    "{\\bf p}_n & = {\\rm LSTM} \\bigl( W_{ip} \\cdot {\\bf i}_n + W_{pp} \\cdot {\\bf p}_{n-1} \\bigr), \\\\\n",
    "{\\bf q}_1 & = {\\rm LSTM} \\bigl( W_{pq} \\cdot {\\bf p}_{|{\\bf w}|} \\bigr), \\\\\n",
    "{\\bf q}_m & = {\\rm LSTM} \\bigl( W_{yq} \\cdot {\\bf y}_{m-1} + W_{qq} \\cdot {\\bf q}_{m-1} \\bigr), \\\\\n",
    "{\\bf j}_m & = \\tanh \\bigl( W_{qj} \\cdot {\\bf q}_m \\bigr), \\\\\n",
    "{\\bf y}_m & = {\\rm softmax} \\bigl( W_{jy} \\cdot {\\bf j}_m \\bigr).\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "## prob. of sentence connection\n",
    "\n",
    "$ \\begin{align}\n",
    "\\log {\\rm Pr} \\bigl( {\\bf w} \\bigr) & = \\sum_{n=1}^{|{\\bf w}|} \\log {\\rm Pr} \\bigl( w_n \\ \\big| \\ w_1, w_2, \\cdots, w_{n-1} \\bigr) \\\\\n",
    "& = \\sum_{n=1}^{|{\\bf w}|} \\log {\\bf y}_n\\big[ {\\rm index} \\bigl( w_n \\bigr) \\big]\n",
    "\\end{align} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f=codecs.open('/home/hitoshi/train1000.ja','r','utf_8'); ja_data = f.read(); f.close();\n",
    "f = open(\"/home/hitoshi/train1000.en\"); en_data = f.read(); f.close();\n",
    "f=codecs.open('/home/hitoshi/test10.ja','r','utf_8'); ja_test = f.read(); f.close();\n",
    "f = open(\"/home/hitoshi/test10.en\"); en_test = f.read(); f.close();\n",
    "\n",
    "en_sentences = [en_sentence.split(\" \") for en_sentence in en_data.split(\"\\n\")]\n",
    "en_sentences_test = [en_sentence.split(\" \") for en_sentence in en_test.split(\"\\n\")]\n",
    "en_words_set = set(sum(en_sentences,[])).union(set(sum(en_sentences_test,[])))\n",
    "en_words_set = en_words_set.difference([''])\n",
    "\n",
    "ja_sentences = [mt.parse(ja_sentence.replace(\" \",\"\").encode('utf-8')).decode('utf-8').split(\" \")[0:-1] for ja_sentence in ja_data.split(\"\\n\")]\n",
    "ja_sentences_test = [mt.parse(ja_sentence.replace(\" \",\"\").encode('utf-8')).decode('utf-8').split(\" \")[0:-1] for ja_sentence in ja_test.split(\"\\n\")]\n",
    "ja_words_set = set(sum(ja_sentences,[])).union(set(sum(ja_sentences_test,[])))\n",
    "ja_words_set = ja_words_set.difference([''])\n",
    "\n",
    "ja_word_to_id = dict(zip(ja_words_set, range(len(ja_words_set))))\n",
    "en_word_to_id = dict(zip(en_words_set, range(len(en_words_set))))\n",
    "id_to_ja_word = dict(zip(range(len(ja_words_set)),ja_words_set))\n",
    "id_to_en_word = dict(zip(range(len(en_words_set)),en_words_set))\n",
    "\n",
    "ja_test_sentences = ja_test.split(\"\\n\")\n",
    "en_test_sentences = en_test.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = len(ja_word_to_id) + 1\n",
    "SRC_EMBED_SIZE = len(ja_word_to_id) + 1\n",
    "HIDDEN_SIZE = 100\n",
    "TRG_VOCAB_SIZE = len(en_word_to_id) + 2\n",
    "TRG_EMBED_SIZE = len(en_word_to_id) + 2\n",
    "END_OF_SENTENCE = len(en_word_to_id)\n",
    "\n",
    "model = FunctionSet(\n",
    "  w_xi = F.EmbedID(SRC_VOCAB_SIZE, SRC_EMBED_SIZE), # 入力層(one-hot) -> 入力埋め込み層\n",
    "  w_ip = F.Linear(SRC_EMBED_SIZE, 4 * HIDDEN_SIZE), # 入力埋め込み層 -> 入力隠れ層\n",
    "  w_pp = F.Linear(HIDDEN_SIZE, 4 * HIDDEN_SIZE), # 入力隠れ層 -> 入力隠れ層\n",
    "  w_pq = F.Linear(HIDDEN_SIZE, 4 * HIDDEN_SIZE), # 入力隠れ層 -> 出力隠れ層\n",
    "  w_yq = F.EmbedID(TRG_VOCAB_SIZE, 4 * HIDDEN_SIZE), # 出力層(one-hot) -> 出力隠れ層\n",
    "  w_qq = F.Linear(HIDDEN_SIZE, 4 * HIDDEN_SIZE), # 出力隠れ層 -> 出力隠れ層\n",
    "  w_qj = F.Linear(HIDDEN_SIZE, TRG_EMBED_SIZE), # 出力隠れ層 -> 出力埋め込み層\n",
    "  w_jy = F.Linear(TRG_EMBED_SIZE, TRG_VOCAB_SIZE), # 出力隠れ層 -> 出力隠れ層\n",
    ")\n",
    "\n",
    "# src_sentence: 翻訳したい単語列 e.g. ['彼', 'は', '走る']\n",
    "# trg_sentence: 正解の翻訳を表す単語列 e.g. ['he', 'runs']\n",
    "def forward(src_sentence, trg_sentence, model, training):\n",
    "  src_sentence = [ja_word_to_id[word] if word in ja_words_set else SRC_VOCAB_SIZE-1 for word in src_sentence]\n",
    "  trg_sentence = [en_word_to_id[word] if word in en_words_set else TRG_VOCAB_SIZE-1 for word in trg_sentence] + [END_OF_SENTENCE]\n",
    "  c = Variable(np.zeros((1, HIDDEN_SIZE), dtype=np.float32))  # Initialization of LSTM inner state\n",
    "  x = Variable(np.array([END_OF_SENTENCE], dtype=np.int32))  # encoder\n",
    "  i = F.tanh(model.w_xi(x))\n",
    "  c, p = F.lstm(c, model.w_ip(i))\n",
    "  for word in reversed(src_sentence):\n",
    "    x = Variable(np.array([[word]], dtype=np.int32)) # next input layer\n",
    "    i = F.tanh(model.w_xi(x))\n",
    "    c, p = F.lstm(c, model.w_ip(i) + model.w_pp(p))\n",
    "  c, q = F.lstm(c, model.w_pq(p)) # encoder -> decoder\n",
    "  if training: # decoder\n",
    "    accum_loss = np.zeros((), dtype=np.float32)\n",
    "    for word in trg_sentence:\n",
    "      j = F.tanh(model.w_qj(q))\n",
    "      y = model.w_jy(j)\n",
    "      t = Variable(np.array([word], dtype=np.int32))\n",
    "      accum_loss = accum_loss + F.softmax_cross_entropy(y, t)\n",
    "      c, q = F.lstm(c, model.w_yq(t)+ model.w_qq(q))\n",
    "    return accum_loss\n",
    "  else:\n",
    "    # 予測時には翻訳器が生成したyを次回の入力に使い、forwardの結果として生成された単語列を返す。\n",
    "    # yの中で最大の確率を持つ単語を選択していくが、softmaxを取る必要はない。\n",
    "    hyp_sentence = []\n",
    "    while len(hyp_sentence) < 100: # 100単語以上は生成しないようにする\n",
    "      j = F.tanh(model.w_qj(q))\n",
    "      y = model.w_jy(j)\n",
    "      word = y.data.argmax(1)[0]\n",
    "      if word == END_OF_SENTENCE:\n",
    "        break # 終端記号が生成されたので終了\n",
    "      hyp_sentence.append(id_to_en_word[word])\n",
    "      s_y = Variable(np.array([word], dtype=np.int32))\n",
    "      c, q = F.lstm(c, model.w_yq(s_y) + model.w_qq(q))\n",
    "    return hyp_sentence\n",
    "def train(ja_sentences,en_sentences,model):\n",
    "  opt = optimizers.SGD(); opt.setup(model); # Setop optimizer >> opt = optimizers.Adam() is also good!\n",
    "  for (ja_sentence, en_sentence) in zip(ja_sentences,en_sentences):\n",
    "    opt.zero_grads(); # Initialization of grad.\n",
    "    accum_loss = forward(ja_sentence,en_sentence, model, training = True) # calc forward\n",
    "    accum_loss.backward() # calc backprop\n",
    "    opt.clip_grads(10) # Suppression of big grad.\n",
    "    opt.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "  print i\n",
    "  train(ja_sentences,en_sentences,model)\n",
    "  hyp_sentence = forward(ja_test_sentences[0],en_test_sentences[0],model, training = False)\n",
    "  text = \"\"\n",
    "  for w in ja_test_sentences[0]:\n",
    "    text = text + w\n",
    "  print \"=====問題======\",text\n",
    "  print \"=====正解======\",en_test_sentences[0]\n",
    "  print \"=====予測======\",hyp_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,15):\n",
    "  print i\n",
    "  train(ja_sentences,en_sentences,model)\n",
    "  hyp_sentence = forward(ja_test_sentences[1],en_test_sentences[1],model, training = False)\n",
    "  text = \"\"\n",
    "  for w in ja_test_sentences[1]:\n",
    "    text = text + w\n",
    "  print \"=====問題======\",text\n",
    "  print \"=====正解======\",en_test_sentences[1]\n",
    "  print \"=====予測======\",hyp_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
